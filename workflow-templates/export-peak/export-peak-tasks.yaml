# See the NOTICE file distributed with this work for additional information
# regarding copyright ownership.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: export-peak-tasks-v-0.1.1
  annotations:
    workflows.argoproj.io/description: |
      Generate bigbed file from peak bed file
spec:
  entrypoint: export-peak-tasks-dag

  templates:

    - name: export-peak-tasks-dag
      inputs:
        parameters:
          - name: species_name
            value: ""
          - name: assembly_ensembl_name
            value: ""
          - name: epigenome_group_id
            value: ""

          - name: experiment_type
            value: ""
          - name: epigenome_name_contains
            value: ""
          - name: experiment_name_contains
            value: ""
          - name: target_name_contains
            value: ""
          - name: histone_mark_type
            value: ""

          - name: ad_hoc_playground_prefix
            value: ""
          - name: output_prefix_label
            value: ""

          - name: skip
            value: "0"
          - name: limit
            value: "20"

          - name: overwrite-results
            value: "false"

      dag:
        tasks:
          - name: get-export-peak-tasks-url
            template: compute-get-export-peak-tasks-url
            arguments:
              parameters:
                - name: svc-name
                  value: "regulation-pipelines-api-service"
                - name: namespace
                  value: "regulation-pipelines-api"

                - name: species_name
                  value: "{{inputs.parameters.species_name}}"
                - name: assembly_ensembl_name
                  value: "{{inputs.parameters.assembly_ensembl_name}}"
                - name: epigenome_group_id
                  value: "{{inputs.parameters.epigenome_group_id}}"

                - name: experiment_type
                  value: "{{inputs.parameters.experiment_type}}"
                - name: epigenome_name_contains
                  value: "{{inputs.parameters.epigenome_name_contains}}"
                - name: experiment_name_contains
                  value: "{{inputs.parameters.experiment_name_contains}}"
                - name: target_name_contains
                  value: "{{inputs.parameters.target_name_contains}}"
                - name: histone_mark_type
                  value: "{{inputs.parameters.histone_mark_type}}"

                - name: ad_hoc_playground_prefix
                  value: "{{inputs.parameters.ad_hoc_playground_prefix}}"
                - name: output_prefix_label
                  value: "{{inputs.parameters.output_prefix_label}}"

                - name: skip
                  value: "{{inputs.parameters.skip}}"
                - name: limit
                  value: "{{inputs.parameters.limit}}"

          - name: submit-get-request-from-reg-pipelines-api
            depends: "get-export-peak-tasks-url"
            templateRef:
              name: get-request-from-reg-pipelines-api-v1-submittable
              template: get-request-from-reg-pipelines-api
            arguments:
              parameters:
                - name: url
                  value: "{{tasks.get-export-peak-tasks-url.outputs.result}}"

          - name: submit-export-peak-tasks
            depends: "submit-get-request-from-reg-pipelines-api"
            template: export-peak-tasks-dag
            arguments:
              parameters:
                - name: export-peak-tasks
                  value: "{{tasks.submit-get-request-from-reg-pipelines-api.outputs.result}}"
                - name: overwrite-results
                  value: "{{inputs.parameters.overwrite-results}}"
                - name: kubeconfig-path
                  value: "{{inputs.parameters.kubeconfig-path}}"


    - name: compute-get-export-peak-tasks-url
      inputs:
        parameters:
          - name: svc-name
          - name: namespace
          - name: species_name
          - name: assembly_ensembl_name
          - name: epigenome_group_id
          - name: experiment_type
          - name: epigenome_name_contains
          - name: experiment_name_contains
          - name: target_name_contains
          - name: histone_mark_type
          - name: ad_hoc_playground_prefix
          - name: output_prefix_label
          - name: skip
          - name: limit
      script:
        image: dockerhub.ebi.ac.uk/ensreg/workflows/container-images/python-wf-helper:3.11.7_0.1.0
        command: [ python ]
        source: |
          import sys
          import os
          import urllib.parse

          def main() -> int:

            query_params = {
              "species_name": f"{{inputs.parameters.species_name}}",
              "assembly_ensembl_accession": f"{{inputs.parameters.assembly_ensembl_name}}",
              "epigenome_group_id": f"{{inputs.parameters.epigenome_group_id}}",
              "experiment_type": f"{{inputs.parameters.experiment_type}}",
              "epigenome_name_contains": f"{{inputs.parameters.epigenome_name_contains}}",
              "experiment_name_contains": f"{{inputs.parameters.experiment_name_contains}}",
              "target_name_contains": f"{{inputs.parameters.target_name_contains}}",
              "histone_mark_type": f"{{inputs.parameters.histone_mark_type}}",
              "ad_hoc_playground_prefix": f"{{inputs.parameters.ad_hoc_playground_prefix}}",
              "output_prefix_label": f"{{inputs.parameters.output_prefix_label}}",
              "skip": f"{{inputs.parameters.skip}}",
              "limit": f"{{inputs.parameters.limit}}",
            }

            parsed_query_params = "".join(
              [
                f"&{key}={value}" for key, value in query_params.items()
                if value
              ]
            )

            url_str = (f"http://{{inputs.parameters.svc-name}}."
                  f"{{inputs.parameters.namespace}}."
                  "svc.cluster.local"
                  ":80"
                  "/api/v1"
                  "/export_peak_tasks"
                  f"?{parsed_query_params}"
            )

            print(urllib.parse.quote(url_str, safe=":/?=&"))

            return 0

          if __name__ == '__main__':
              sys.exit(main())

    - name: export-peak-tasks-dag
      parallelism: 20
      inputs:
        parameters:
          - name: export-peak-tasks
          - name: overwrite-results
      dag:
        tasks:
          - name: submit-bed-to-bigbed
            template: submit-bed-to-bigbed-dag
            arguments:
              parameters:
                - name: task-payload
                  value: "{{item}}"
                - name: experiment-id
                  value: "{{item.experiment_id}}"
                - name: assembly-id
                  value: "{{item.assembly_id}}"
                - name: chrom-sizes-s3-key
                  value: "{{item.chrom_sizes_s3_key}}"
                - name: out-s3-key
                  value: "{{item.out_s3_prefix}}{{item.out_basename}}.bb"
                - name: bed-s3-key
                  value: "{{item.bed_s3_key}}"
                - name: bed-file-size
                  value: "{{item.bed_file_size}}"
                - name: bed-md5sum
                  value: "{{item.bed_md5sum}}"
                - name: overwrite-results
                  value: "{{inputs.parameters.overwrite-results}}"
                - name: force-update-task-markers
                  value: "false"
                - name: bed-type
                  value: "{{item.bed_type}}"
            withParam: "{{inputs.parameters.export-peak-tasks}}"


    - name: submit-bed-to-bigbed-dag
      inputs:
        parameters:
          - name: task-payload
          - name: experiment-id
          - name: assembly-id
          - name: chrom-sizes-s3-key
          - name: out-s3-key
          - name: bed-s3-key
          - name: bed-file-size
          - name: bed-md5sum
          - name: bed-type
          - name: overwrite-results
          - name: force-update-task-markers
            value: "false"

      dag:
        tasks:
          - name: compute-export-peak-task-marker-name
            templateRef:
              name: compute-task-marker-name-v1-submittable
              template: compute-task-marker-name
            arguments:
              parameters:
                - name: task-payload
                  value: "{{inputs.parameters.task-payload}}"
                - name: task-type
                  value: "export-peak"

          - name: check-if-task-marker-exists
            depends: compute-export-peak-task-marker-name
            templateRef:
              name: check-s3-object-exists-v-0.1.0
              template: check-s3-object-exists
            arguments:
              parameters:
                - name: s3-key
                  value: "{{tasks.compute-export-peak-task-marker-name.outputs.result}}"

          - name: check-if-bigbed-file-already-in-s3-bucket
            templateRef:
              name: check-s3-object-exists-v-0.1.0
              template: check-s3-object-exists
            arguments:
              parameters:
                - name: s3-key
                  value: "{{inputs.parameters.out-s3-key}}"

          - name: check-work-avoidance-consistency
            depends: "check-if-task-marker-exists && check-if-bigbed-file-already-in-s3-bucket"
            templateRef:
              name: resolve-work-avoidance-v-0.1.0
              template: resolve-work-avoidance
            arguments:
              parameters:
                - name: marker-exists
                  value: "{{tasks.check-if-task-marker-exists.outputs.parameters.s3-object-exists}}"
                - name: output-artifact-exists
                  value: "{{tasks.check-if-bigbed-file-already-in-s3-bucket.outputs.parameters.s3-object-exists}}"
                - name: overwrite-results
                  value: "{{inputs.parameters.overwrite-results}}"

          - name: bed-to-bigbed
            depends: check-work-avoidance-consistency
            templateRef:
              name: bed-to-bigbed-v-0.1.0
              template: bed-to-bigbed
            arguments:
              parameters:
                - name: experiment-id
                  value: "{{inputs.parameters.experiment-id}}"
                - name: assembly-id
                  value: "{{inputs.parameters.assembly-id}}"
                - name: bed-s3-key
                  value: "{{inputs.parameters.bed-s3-key}}"
                - name: chrom-sizes-s3-key
                  value: "{{inputs.parameters.chrom-sizes-s3-key}}"
                - name: bigbed-s3-key
                  value: "{{inputs.parameters.out-s3-key}}"
                - name: bed-type
                  value: "{{inputs.parameters.bed-type}}"

          - name: update-export-peak-task-marker
            depends: "bed-to-bigbed || check-work-avoidance-consistency.Failed"
            when: >-
              ( ( {{inputs.parameters.force-update-task-markers}} == true
              && {{tasks.check-work-avoidance-consistency.status}} == Failed )
              || {{tasks.bed-to-bigbed.status}} == Succeeded )
            templateRef:
              name: update-task-marker-v-0.1.0
              template: update-task-marker
            arguments:
              parameters:
                - name: task-marker-s3-key
                  value: "{{tasks.compute-export-peak-task-marker-name.outputs.result}}"
                - name: task-payload
                  value: "{{inputs.parameters.task-payload}}"
 
