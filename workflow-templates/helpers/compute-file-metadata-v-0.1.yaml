# See the NOTICE file distributed with this work for additional information
# regarding copyright ownership.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: compute-file-metadata-v-0.1.0
spec:
  entrypoint: compute-file-metadata

  templates:
    - name: compute-file-metadata
      retryStrategy:
        limit: "5"
        backoff:
          duration: "10s"
          factor: "2"
        retryPolicy: "OnError"
      inputs:
        parameters:
          - name: s3-key
          - name: filename
        artifacts:
          - name: input-file
            path: /home/ensreg/workdir/{{inputs.parameters.filename}}
            s3:
              key: "{{inputs.parameters.s3-key}}"
      script:
        image: dockerhub.ebi.ac.uk/ensreg/workflows/container-images/python-wf-helper:3.11.7_0.1.0
        command: [ python ]
        source: |
          import sys
          import os
          import hashlib
          import json
          
          def main() -> int:
          
            filename = "{{inputs.parameters.filename}}"
          
            file_size = os.path.getsize(filename)
          
            with open('file_size.txt', 'w') as f:
              f.write(str(file_size))
          
            CHUNK_SIZE = 8192
            # Reads the file 8192 (or 2¹³) bytes at a time
            # instead of all at once with f.read() to use less memory.
            with open(filename, "rb") as f:
                file_hash = hashlib.md5()
                chunk = f.read(CHUNK_SIZE)
                while chunk:
                    file_hash.update(chunk)
                    chunk = f.read(CHUNK_SIZE)
          
            md5sum = file_hash.hexdigest()
          
            with open('md5sum.txt', 'w') as f:
              f.write(md5sum)
          
            return 0
          
          if __name__ == '__main__':
              sys.exit(main())

      outputs:
        parameters:
          - name: file-size
            valueFrom:
              path: file_size.txt
          - name: md5sum
            valueFrom:
              path: md5sum.txt