apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: peak-calling-atac-seq-v1-submittable
spec:
  entrypoint: peak-calling-atac-seq

  imagePullSecrets:
    - name: ghcr-pull-token

  templates:
    - name: peak-calling-atac-seq
      parallelism: 3
      inputs:
        parameters:
          - name: peak_calling_tasks
          - name: masked_regions_prefix
          - name: masked_regions_filename
          - name: overwrite-results
          - name: kubeconfig-path
      dag:
        tasks:
          - name: execute-peak-calling-task
            template: peak-calling-task
            arguments:
              parameters:
                - name: task-payload
                  value: "{{item}}"
                - name: experiment_id
                  value: "{{item.experiment_id}}"
                - name: assembly_id
                  value: "{{item.assembly_id}}"
                - name: out_basename
                  value: "{{item.out_basename}}"
                - name: out_s3_prefix
                  value: "{{item.out_s3_prefix}}"
                - name: alignment_files_total_size
                  value: "{{item.alignment_files_total_size}}"
                - name: signals
                  value: "{{item.signals}}"
                - name: masked_regions_prefix
                  value: "{{inputs.parameters.masked_regions_prefix}}"
                - name: masked_regions_filename
                  value: "{{inputs.parameters.masked_regions_filename}}"
                - name: overwrite-results
                  value: "{{inputs.parameters.overwrite-results}}"
                - name: kubeconfig-path
                  value: "{{inputs.parameters.kubeconfig-path}}"
                - name: force-update-task-markers
                  value: "false"
            withParam: "{{inputs.parameters.peak_calling_tasks}}"


    - name: peak-calling-task
      parallelism: 2
      inputs:
        parameters:
          - name: task-payload
          - name: experiment_id
          - name: assembly_id
          - name: out_basename
          - name: out_s3_prefix
          - name: alignment_files_total_size
          - name: signals
          - name: masked_regions_prefix
          - name: masked_regions_filename
          - name: kubeconfig-path
          - name: overwrite-results
          - name: force-update-task-markers
            value: "false"

      dag:
        tasks:
          - name: compute-peak-calling-task-marker-name
            templateRef:
              name: compute-task-marker-name-v1-submittable
              template: compute-task-marker-name
            arguments:
              parameters:
                - name: task-payload
                  value: "{{inputs.parameters.task-payload}}"
                - name: task-type
                  value: "peak-calling"

          - name: check-if-task-marker-exists
            depends: compute-peak-calling-task-marker-name
            templateRef:
              name: check-s3-object-exists-v1-submittable
              template: check-s3-object-exists
            arguments:
              parameters:
                - name: s3-key
                  value: "{{tasks.compute-peak-calling-task-marker-name.outputs.result}}"

          - name: check-if-bed-file-already-in-s3-bucket
            templateRef:
              name: check-s3-object-exists-v1-submittable
              template: check-s3-object-exists
            arguments:
              parameters:
                - name: s3-key
                  value: "{{inputs.parameters.out_s3_prefix}}{{inputs.parameters.out_basename}}.bed"

          - name: check-work-avoidance-consistency
            depends: "check-if-task-marker-exists && check-if-bed-file-already-in-s3-bucket"
            templateRef:
              name: resolve-work-avoidance-v1-submittable
              template: resolve-work-avoidance
            arguments:
              parameters:
                - name: marker-exists
                  value: "{{tasks.check-if-task-marker-exists.outputs.parameters.s3-object-exists}}"
                - name: output-artifact-exists
                  value: "{{tasks.check-if-bed-file-already-in-s3-bucket.outputs.parameters.s3-object-exists}}"
                - name: overwrite-results
                  value: "{{inputs.parameters.overwrite-results}}"

          - name: get-peak-calling-pvc-size
            depends: check-work-avoidance-consistency.Succeeded
            when: "{{tasks.check-work-avoidance-consistency.outputs.parameters.avoid-work}} == false"
            templateRef:
              name: compute-pvc-size-v1-submittable
              template: compute-pvc-size
            arguments:
              parameters:
                - name: associated_files_size
                  value: "{{inputs.parameters.alignment_files_total_size}}"
                - name: size_factor
                  value: "3.5"

          - name: create-peak-calling-pvc
            depends: "get-peak-calling-pvc-size.Succeeded"
            templateRef:
              name: create-pvc-kubectl-v1-submittable
              template: create-pvc-kubectl
            arguments:
              parameters:
                - name: pvc-size
                  value: "{{tasks.get-peak-calling-pvc-size.outputs.parameters.pvc-size-formatted}}"
                - name: kubeconfig-path
                  value: "{{inputs.parameters.kubeconfig-path}}"

          - name: execute-sort-alignments-by-queryname
            depends: create-peak-calling-pvc
            template: sort-alignments-by-queryname
            arguments:
              parameters:
                - name: sort_alignment_params
                  value: "{{item}}"
                # PVC in which to store so_queryname bams
                - name: out-pvc-name
                  value: "{{tasks.create-peak-calling-pvc.outputs.parameters.pvc-name}}"
                - name: kubeconfig-path
                  value: "{{inputs.parameters.kubeconfig-path}}"
            withParam: "{{inputs.parameters.signals}}"

          - name: execute-genrich-atac-seq
            depends: execute-sort-alignments-by-queryname
            template: genrich-atac-seq
            arguments:
              parameters:
                - name: masked_regions_prefix
                  value: "{{inputs.parameters.masked_regions_prefix}}"
                - name: masked_regions_filename
                  value: "{{inputs.parameters.masked_regions_filename}}"
                - name: out_bed_filename
                  value: "{{inputs.parameters.out_basename}}.bed"
                - name: out_pileup_filename
                  value: "{{inputs.parameters.out_basename}}_pileup.log"
                - name: out_pq_file_filename
                  value: "{{inputs.parameters.out_basename}}_pqval.log"
                - name: out_log_filename
                  value: "{{inputs.parameters.out_basename}}_run.log"
                - name: out_s3_prefix
                  value: "{{inputs.parameters.out_s3_prefix}}"
                - name: signals-pvc
                  value: "{{tasks.create-peak-calling-pvc.outputs.parameters.pvc-name}}"

          - name: execute-compute-bed-file-metadata
            depends: execute-genrich-atac-seq
            templateRef:
              name: compute-file-metadata-pvc-callable-v1-submittable
              template: compute-file-metadata-pvc-callable
            arguments:
              parameters:
                - name: pvc-name
                  value: "{{tasks.create-peak-calling-pvc.outputs.parameters.pvc-name}}"
                - name: filename
                  value: "{{inputs.parameters.out_basename}}.bed"

          - name: execute-post-peak-file
            depends: execute-compute-bed-file-metadata
            template: post-peak-file
            arguments:
              parameters:
                - name: s3-key
                  value: "{{inputs.parameters.out_s3_prefix}}{{inputs.parameters.out_basename}}.bed"
                - name: basename
                  value: "{{inputs.parameters.out_basename}}"
                - name: file-size
                  value: "{{tasks.execute-compute-bed-file-metadata.outputs.parameters.file-size}}"
                - name: md5sum
                  value: "{{tasks.execute-compute-bed-file-metadata.outputs.parameters.md5sum}}"
                - name: experiment-id
                  value: "{{inputs.parameters.experiment_id}}"
                - name: assembly-id
                  value: "{{inputs.parameters.assembly_id}}"

          - name: update-peak-calling-task-marker
            depends: "execute-post-peak-file || check-work-avoidance-consistency.Failed"
            when: >-
              ( ( {{inputs.parameters.force-update-task-markers}} == true 
              && {{tasks.check-work-avoidance-consistency.status}} == Failed ) 
              || {{tasks.execute-post-peak-file.status}} == Succeeded )
            templateRef:
              name: update-task-marker-v1-submittable
              template: update-task-marker
            arguments:
              parameters:
                - name: task-marker-s3-key
                  value: "{{tasks.compute-peak-calling-task-marker-name.outputs.result}}"
                - name: task-payload
                  value: "{{inputs.parameters.task-payload}}"

          - name: delete-peak-calling-pvc
            depends: execute-compute-bed-file-metadata
            templateRef:
              name: delete-patch-pvc-kubectl-v1-submittable
              template: delete-patch-pvc-kubectl
            arguments:
              parameters:
                - name: pvc-name
                  value: "{{tasks.create-peak-calling-pvc.outputs.parameters.pvc-name}}"
                - name: kubeconfig-path
                  value: "{{inputs.parameters.kubeconfig-path}}"


    - name: sort-alignments-by-queryname
      inputs:
        parameters:
          # Bam file as input
          - name: sort_alignment_params
          # PVC in which to store bedgraph
          - name: out-pvc-name
          - name: kubeconfig-path
      dag:
        tasks:
          - name: get-pvc-size
            templateRef:
              name: compute-pvc-size-v1-submittable
              template: compute-pvc-size
            arguments:
              parameters:
                - name: associated_files_size
                  value: "{{=jsonpath(inputs.parameters.sort_alignment_params, '$.file_size')}}"
                - name: size_factor
                  value: "1.5"

          - name: execute-sort-alignment-by-queryname
            depends: get-pvc-size
            templateRef:
              name: sort-alignment-by-queryname-pvc-callable-v1-submittable
              template: sort-alignment-by-queryname-pvc-callable
            arguments:
              parameters:
                - name: basename
                  value: "{{=jsonpath(inputs.parameters.sort_alignment_params, '$.basename')}}"
                - name: filename
                  value: "{{=jsonpath(inputs.parameters.sort_alignment_params, '$.filename')}}"
                - name: s3_key
                  value: "{{=jsonpath(inputs.parameters.sort_alignment_params, '$.s3_key')}}"
                - name: pvc-size
                  value: "{{tasks.get-pvc-size.outputs.parameters.pvc-size-formatted}}"
                - name: kubeconfig-path
                  value: "{{inputs.parameters.kubeconfig-path}}"
                - name: out-pvc-name
                  value: "{{inputs.parameters.out-pvc-name}}"


    - name: post-peak-file
      retryStrategy:
        limit: "5"
        retryPolicy: "OnError"
      inputs:
        parameters:
          - name: s3-key
          - name: basename
          - name: file-size
          - name: md5sum
          - name: experiment-id
          - name: assembly-id
      script:
        image: ghcr.io/daugo/ensembl-reg-python:latest
        command: [ python ]
        source: |
          import sys
          import os
          import hashlib
          import json
          
          def main() -> int:
          
            post_data = {
              "experiment_id": "{{inputs.parameters.experiment-id}}",
              "assembly_id": "{{inputs.parameters.assembly-id}}",
              "analysis_type": "peaks",
              "file": {
                "file_type": "bed",
                "basename": "{{inputs.parameters.basename}}",
                "size": "{{inputs.parameters.file-size}}",
                "md5sum": "{{inputs.parameters.md5sum}}",
                "imported": False,
                "s3_object": {
                  "key": "{{inputs.parameters.s3-key}}",
                  "bucket_id": "085c4884-d0d6-4725-bda2-7463deed86eb"
                }
              }
            }
          
            print(json.dumps(post_data,  indent=2))
          
            return 0
          
          if __name__ == '__main__':
              sys.exit(main())


    - name: genrich-atac-seq
      retryStrategy:
        limit: "3"
        retryPolicy: "OnError"
      inputs:
        parameters:
          - name: masked_regions_prefix
          - name: masked_regions_filename
          - name: out_bed_filename
          - name: out_pileup_filename
          - name: out_pq_file_filename
          - name: out_log_filename
          - name: out_s3_prefix
          - name: signals-pvc
        artifacts:
          - name: masked-regions-file
            path: /mnt/vol/{{inputs.parameters.masked_regions_filename}}
            s3:
              key: "{{inputs.parameters.masked_regions_prefix}}{{inputs.parameters.masked_regions_filename}}"
      container:
        workingDir: /mnt/vol
        image: ghcr.io/daugo/ensembl-reg-genrich:latest
        command: [ bash, -c ]
        args: [ "ls -altrh \
                && SIGNAL_BAM_FILES=$(ls -m *.bam | xargs echo | sed 's/ //g') \
                && Genrich \
                -t ${SIGNAL_BAM_FILES} \
                -o {{inputs.parameters.out_bed_filename}} \
                -f {{inputs.parameters.out_pileup_filename}} \
                -k {{inputs.parameters.out_pq_file_filename}} \
                -r \
                -j \
                -q 0.1 \
                -s 20 \
                -e MT \
                -E {{inputs.parameters.masked_regions_filename}} \
                -v \
                | tee {{inputs.parameters.out_log_filename}} \
                && ls -altrh" ]
        resources:
          limits:
            cpu: 4900m
            memory: 45Gi
          requests:
            cpu: 4700m
            memory: 41Gi
        volumeMounts:
          - name: workdir
            mountPath: /mnt/vol
      volumes:
        - name: workdir
          persistentVolumeClaim:
            claimName: "{{inputs.parameters.signals-pvc}}"
      outputs:
        artifacts:
          - name: final_bed
            path: /mnt/vol/{{inputs.parameters.out_bed_filename}}
            archive:
              none: { }
            s3:
              key: "{{inputs.parameters.out_s3_prefix}}{{inputs.parameters.out_bed_filename}}"
          - name: pileup
            path: /mnt/vol/{{inputs.parameters.out_pileup_filename}}
            s3:
              key: "{{inputs.parameters.out_s3_prefix}}{{inputs.parameters.out_pileup_filename}}.tgz"
          - name: pq_file
            path: /mnt/vol/{{inputs.parameters.out_pq_file_filename}}
            s3:
              key: "{{inputs.parameters.out_s3_prefix}}{{inputs.parameters.out_pq_file_filename}}.tgz"
          - name: log_file
            path: /mnt/vol/{{inputs.parameters.out_log_filename}}
            archive:
              none: { }
            s3:
              key: "{{inputs.parameters.out_s3_prefix}}{{inputs.parameters.out_log_filename}}"
