apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: genrich-values-v2-submittable
#  annotations:
#    workflows.argoproj.io/description: |
#      Generate Genrich files for signal generation and Genrich re-runs
spec:
  entrypoint: genrich-values
  imagePullSecrets:
    - name: ghcr-pull-token
  podGC:
    strategy: OnPodSuccess

  templates:
    - name: genrich-values
      parallelism: 10
      inputs:
        parameters:
          - name: peak-calling-tasks
          - name: masked-regions-s3-key
          - name: controls-mapping-threshold
          - name: chipmentation
          - name: overwrite-results
          - name: kubeconfig-path
      dag:
        tasks:
          - name: execute-peak-calling-task
            template: peak-calling-task
            arguments:
              parameters:
                - name: task-payload
                  value: "{{item}}"
                - name: chipmentation
                  value: "{{inputs.parameters.chipmentation}}"
                - name: masked-regions-s3-key
                  value: "{{inputs.parameters.masked-regions-s3-key}}"
                - name: overwrite-results
                  value: "{{inputs.parameters.overwrite-results}}"
                - name: controls-mapping-threshold
                  value: "{{inputs.parameters.controls-mapping-threshold}}"
                - name: kubeconfig-path
                  value: "{{inputs.parameters.kubeconfig-path}}"
            withParam: "{{inputs.parameters.peak-calling-tasks}}"


    - name: peak-calling-task
      parallelism: 8
      inputs:
        parameters:
          - name: task-payload
          - name: masked-regions-s3-key
          - name: controls-mapping-threshold
          - name: chipmentation
          - name: overwrite-results
          - name: kubeconfig-path
      dag:
        tasks:
          - name: compute-peak-calling-task-marker-name
            templateRef:
              name: compute-task-marker-name-v1-submittable
              template: compute-task-marker-name
            arguments:
              parameters:
                - name: task-payload
                  value: "{{inputs.parameters.task-payload}}"
                - name: task-type
                  value: "genrich-values"

          - name: check-if-task-marker-exists
            depends: compute-peak-calling-task-marker-name
            templateRef:
              name: check-s3-object-exists-v1-submittable
              template: check-s3-object-exists
            arguments:
              parameters:
                - name: s3-key
                  value: "{{tasks.compute-peak-calling-task-marker-name.outputs.result}}"

          - name: check-if-pq-values-and-pileups-already-in-s3-bucket
            templateRef:
              name: check-if-s3-objects-exist-v1-submittable
              template: check-if-s3-objects-exist
            arguments:
              parameters:
                - name: s3-keys
                  value: |
                    [
                      "{{=jsonpath(inputs.parameters['task-payload'], '$.out_s3_prefix')}}pq-values-{{=jsonpath(inputs.parameters['task-payload'], '$.out_basename')}}.txt.tgz",
                      "{{=jsonpath(inputs.parameters['task-payload'], '$.out_s3_prefix')}}pileups-{{=jsonpath(inputs.parameters['task-payload'], '$.out_basename')}}.txt.tgz"
                    ]

          - name: check-work-avoidance-consistency
            depends: "check-if-task-marker-exists && check-if-pq-values-and-pileups-already-in-s3-bucket"
            templateRef:
              name: resolve-work-avoidance-v1-submittable
              template: resolve-work-avoidance
            arguments:
              parameters:
                - name: marker-exists
                  value: "{{tasks.check-if-task-marker-exists.outputs.parameters.s3-object-exists}}"
                - name: output-artifact-exists
                  value: "{{tasks.check-if-pq-values-and-pileups-already-in-s3-bucket.outputs.parameters.check-response}}"
                - name: overwrite-results
                  value: "{{inputs.parameters.overwrite-results}}"


          - name: execute-signals-samtools-sort-tasks
            depends: check-work-avoidance-consistency.Succeeded
            when: "{{tasks.check-work-avoidance-consistency.outputs.parameters.avoid-work}} == false"
            template: dag-samtools-sort-tasks
            arguments:
              parameters:
                - name: alignments-info
                  value: "{{=toJson(jsonpath(inputs.parameters['task-payload'], '$.signals'))}}"
                - name: kubeconfig-path
                  value: "{{inputs.parameters.kubeconfig-path}}"

          - name: check-if-any-controls-present
            depends: check-work-avoidance-consistency.Succeeded
            when: "{{tasks.check-work-avoidance-consistency.outputs.parameters.avoid-work}} == false"
            templateRef:
              name: check-if-field-present-and-not-empty-v1-submittable
              template: check-if-field-present-and-not-empty
            arguments:
              parameters:
                - name: field-name
                  value: "controls"
                - name: task-payload
                  value: "{{inputs.parameters.task-payload}}"

          - name: execute-controls-samtools-sort-tasks
            depends: check-if-any-controls-present.Succeeded
            when: "{{tasks.check-if-any-controls-present.outputs.parameters.controls-present}} == true"
            template: dag-samtools-sort-tasks
            arguments:
              parameters:
                - name: alignments-info
                  value: "{{=toJson(jsonpath(inputs.parameters['task-payload'], '$.controls'))}}"
                - name: kubeconfig-path
                  value: "{{inputs.parameters.kubeconfig-path}}"

          - name: get-genrich-inputs-only-signals
            depends: check-if-any-controls-present.Succeeded
            when: "{{tasks.check-if-any-controls-present.outputs.parameters.controls-present}} == false"
            template: resolve-genrich-inputs
            arguments:
              parameters:
                - name: signals-payload
                  value: "{{=toJson(jsonpath(inputs.parameters['task-payload'], '$.signals'))}}"
                - name: similarity-threshold
                  value: "{{inputs.parameters.controls-mapping-threshold}}"


          - name: get-genrich-inputs-with-controls
            depends: check-if-any-controls-present.Succeeded
            when: "{{tasks.check-if-any-controls-present.outputs.parameters.controls-present}} == true"
            template: resolve-genrich-inputs
            arguments:
              parameters:
                - name: signals-payload
                  value: "{{=toJson(jsonpath(inputs.parameters['task-payload'], '$.signals'))}}"
                - name: controls-payload
                  value: "{{=toJson(jsonpath(inputs.parameters['task-payload'], '$.controls'))}}"
                - name: similarity-threshold
                  value: "{{inputs.parameters.controls-mapping-threshold}}"

          - name: get-peak-calling-pvc-size
            depends: >-
              ( ( get-genrich-inputs-only-signals.Succeeded
               || get-genrich-inputs-with-controls.Succeeded )
               && execute-signals-samtools-sort-tasks.Succeeded )
            templateRef:
              name: compute-pvc-size-v1-submittable
              template: compute-pvc-size
            arguments:
              parameters:
                - name: associated_files_size
                  value: "{{=jsonpath(inputs.parameters['task-payload'], '$.alignment_files_total_size')}}"
                - name: size_factor
                  value: "2.5"

          - name: create-peak-calling-pvc
            depends: get-peak-calling-pvc-size.Succeeded
            templateRef:
              name: create-pvc-kubectl-v1-submittable
              template: create-pvc-kubectl
            arguments:
              parameters:
                - name: pvc-size
                  value: "{{tasks.get-peak-calling-pvc-size.outputs.parameters.pvc-size-formatted}}"
                - name: kubeconfig-path
                  value: "{{inputs.parameters.kubeconfig-path}}"

          - name: copy-from-signals-samtools-sort-tasks-pvcs-to-peak-calling-task-pvc
            depends: create-peak-calling-pvc.Succeeded && execute-signals-samtools-sort-tasks.Succeeded
            template: dag-copy-sort-alignments-pvcs-to-peak-calling-pvc
            arguments:
              parameters:
                - name: source-pvcs
                  value: "{{tasks.execute-signals-samtools-sort-tasks.outputs.parameters.pvcs-names}}"
                - name: destination-pvc
                  value: "{{tasks.create-peak-calling-pvc.outputs.parameters.pvc-name}}"

          - name: copy-from-controls-samtools-sort-tasks-pvcs-to-peak-calling-task-pvc
            depends: create-peak-calling-pvc.Succeeded && execute-controls-samtools-sort-tasks.Succeeded
            template: dag-copy-sort-alignments-pvcs-to-peak-calling-pvc
            arguments:
              parameters:
                - name: source-pvcs
                  value: "{{tasks.execute-controls-samtools-sort-tasks.outputs.parameters.pvcs-names}}"
                - name: destination-pvc
                  value: "{{tasks.create-peak-calling-pvc.outputs.parameters.pvc-name}}"

          - name: delete-signals-samtools-sort-tasks-pvcs
            depends: copy-from-signals-samtools-sort-tasks-pvcs-to-peak-calling-task-pvc.Succeeded
            templateRef:
              name: delete-patch-pvc-kubectl-v1-submittable
              template: delete-patch-pvc-kubectl
            arguments:
              parameters:
                - name: pvc-name
                  value: "{{item}}"
                - name: kubeconfig-path
                  value: "{{inputs.parameters.kubeconfig-path}}"
            withParam: "{{tasks.execute-signals-samtools-sort-tasks.outputs.parameters.pvcs-names}}"

          - name: delete-controls-samtools-sort-tasks-pvcs
            depends: copy-from-controls-samtools-sort-tasks-pvcs-to-peak-calling-task-pvc
            templateRef:
              name: delete-patch-pvc-kubectl-v1-submittable
              template: delete-patch-pvc-kubectl
            arguments:
              parameters:
                - name: pvc-name
                  value: "{{item}}"
                - name: kubeconfig-path
                  value: "{{inputs.parameters.kubeconfig-path}}"
            withParam: "{{tasks.execute-controls-samtools-sort-tasks.outputs.parameters.pvcs-names}}"

          - name: execute-dag-merge-controls
            depends: >-
              get-genrich-inputs-with-controls.Succeeded
              &&
              copy-from-controls-samtools-sort-tasks-pvcs-to-peak-calling-task-pvc.Succeeded
            # TODO: This could be inferred from the merge_tasks list being empty or not.
            # However, performing this check using expr and/or sprig is not working for 'when'
            # checks. Related issue: https://github.com/argoproj/argo-workflows/issues/7576
            # Attempts:
            # when: "{{=tasks.get-genrich-inputs-with-controls.outputs.parameters['merge-tasks'] != '[]'}}"
            # when: "!{{=sprig.empty(tasks[get-genrich-inputs-with-controls].outputs.parameters['merge-tasks'])}}"
            when: "{{tasks.get-genrich-inputs-with-controls.outputs.parameters.merge-tasks-present}} == true"
            template: dag-merge-controls
            arguments:
              parameters:
                - name: controls-merge-tasks
                  value: "{{tasks.get-genrich-inputs-with-controls.outputs.parameters.merge-tasks}}"
                - name: pvc-name
                  value: "{{tasks.create-peak-calling-pvc.outputs.parameters.pvc-name}}"

          - name: get-genrich-values-pvcs-size
            depends: >-
              delete-signals-samtools-sort-tasks-pvcs.Succeeded 
              && 
              ( execute-dag-merge-controls || execute-controls-samtools-sort-tasks.Skipped )
            templateRef:
              name: compute-pvc-size-v1-submittable
              template: compute-pvc-size
            arguments:
              parameters:
                - name: associated_files_size
                  value: "{{=jsonpath(inputs.parameters['task-payload'], '$.alignment_files_total_size')}}"
                - name: size_factor
                  value: "2.8"

          - name: create-pq-values-pvc
            depends: get-genrich-values-pvcs-size.Succeeded
            templateRef:
              name: create-pvc-kubectl-v1-submittable
              template: create-pvc-kubectl
            arguments:
              parameters:
                - name: pvc-size
                  value: "{{tasks.get-genrich-values-pvcs-size.outputs.parameters.pvc-size-formatted}}"
                - name: kubeconfig-path
                  value: "{{inputs.parameters.kubeconfig-path}}"

          - name: create-pileups-pvc
            depends: get-genrich-values-pvcs-size.Succeeded
            templateRef:
              name: create-pvc-kubectl-v1-submittable
              template: create-pvc-kubectl
            arguments:
              parameters:
                - name: pvc-size
                  value: "{{tasks.get-genrich-values-pvcs-size.outputs.parameters.pvc-size-formatted}}"
                - name: kubeconfig-path
                  value: "{{inputs.parameters.kubeconfig-path}}"

          - name: execute-genrich-no-controls
            depends: >-
              get-genrich-inputs-only-signals.Succeeded 
              && create-pq-values-pvc.Succeeded 
              && create-pileups-pvc.Succeeded
            templateRef:
              name: genrich-values-cmd-v1-submittable
              template: dag-genrich-values-cmd
            arguments:
              parameters:
                - name: signal_files_formatted_str
                  value: "{{tasks.get-genrich-inputs-only-signals.outputs.parameters.signals_files_formatted_str}}"
                - name: control_files_formatted_str
                  value: "{{tasks.get-genrich-inputs-only-signals.outputs.parameters.controls_files_formatted_str}}"
                - name: chipmentation
                  value: "{{inputs.parameters.chipmentation}}"
                - name: out_pileups_filename
                  value: "pileups-{{=jsonpath(inputs.parameters['task-payload'], '$.out_basename')}}.txt"
                - name: out_pq_values_filename
                  value: "pq-values-{{=jsonpath(inputs.parameters['task-payload'], '$.out_basename')}}.txt"
                - name: out_s3_prefix
                  value: "{{=jsonpath(inputs.parameters['task-payload'], '$.out_s3_prefix')}}"
                - name: out_basename
                  value: "{{=jsonpath(inputs.parameters['task-payload'], '$.out_basename')}}"
                - name: experiment-type
                  value: "{{=jsonpath(inputs.parameters['task-payload'], '$.experiment_type')}}"
                - name: alignments-pvc
                  value: "{{tasks.create-peak-calling-pvc.outputs.parameters.pvc-name}}"
                - name: pq-values-pvc
                  value: "{{tasks.create-pq-values-pvc.outputs.parameters.pvc-name}}"
                - name: pileups-pvc
                  value: "{{tasks.create-pileups-pvc.outputs.parameters.pvc-name}}"
                - name: masked-regions-s3-key
                  value: "{{inputs.parameters.masked-regions-s3-key}}"
                - name: masked-regions-filename
                  value: "{{=sprig.base(inputs.parameters['masked-regions-s3-key'])}}"

          - name: execute-genrich
            depends: >-
              get-genrich-inputs-with-controls.Succeeded 
              && create-pq-values-pvc.Succeeded 
              && create-pileups-pvc.Succeeded
            templateRef:
              name: genrich-values-cmd-v1-submittable
              template: dag-genrich-values-cmd
            arguments:
              parameters:
                - name: signal_files_formatted_str
                  value: "{{tasks.get-genrich-inputs-with-controls.outputs.parameters.signals_files_formatted_str}}"
                - name: control_files_formatted_str
                  value: "{{tasks.get-genrich-inputs-with-controls.outputs.parameters.controls_files_formatted_str}}"
                - name: chipmentation
                  value: "{{inputs.parameters.chipmentation}}"
                - name: out_pileups_filename
                  value: "pileups-{{=jsonpath(inputs.parameters['task-payload'], '$.out_basename')}}.txt"
                - name: out_pq_values_filename
                  value: "pq-values-{{=jsonpath(inputs.parameters['task-payload'], '$.out_basename')}}.txt"
                - name: out_s3_prefix
                  value: "{{=jsonpath(inputs.parameters['task-payload'], '$.out_s3_prefix')}}"
                - name: out_basename
                  value: "{{=jsonpath(inputs.parameters['task-payload'], '$.out_basename')}}"
                - name: experiment-type
                  value: "{{=jsonpath(inputs.parameters['task-payload'], '$.experiment_type')}}"
                - name: alignments-pvc
                  value: "{{tasks.create-peak-calling-pvc.outputs.parameters.pvc-name}}"
                - name: pq-values-pvc
                  value: "{{tasks.create-pq-values-pvc.outputs.parameters.pvc-name}}"
                - name: pileups-pvc
                  value: "{{tasks.create-pileups-pvc.outputs.parameters.pvc-name}}"
                - name: masked-regions-s3-key
                  value: "{{inputs.parameters.masked-regions-s3-key}}"
                - name: masked-regions-filename
                  value: "{{=sprig.base(inputs.parameters['masked-regions-s3-key'])}}"

          - name: delete-peak-calling-pvc
            depends: >-
              execute-genrich-no-controls.Succeeded
              || execute-genrich.Succeeded
            templateRef:
              name: delete-patch-pvc-kubectl-v1-submittable
              template: delete-patch-pvc-kubectl
            arguments:
              parameters:
                - name: pvc-name
                  value: "{{tasks.create-peak-calling-pvc.outputs.parameters.pvc-name}}"
                - name: kubeconfig-path
                  value: "{{inputs.parameters.kubeconfig-path}}"

          - name: output-files-post-processing
            depends: >-
              execute-genrich-no-controls.Succeeded
              || execute-genrich.Succeeded
            template: dag-output-files-post-processing
            arguments:
              parameters:
                - name: pvc-name
                  value: "{{item.pvc-name}}"
                - name: task-payload
                  value: "{{inputs.parameters.task-payload}}"
                - name: analysis-type-info
                  value: "{{item}}"
            withParam: >-
              [
                { "analysis_type": "genrich_pq_values",
                  "prefix": "pq-values-",
                  "suffix": ".txt",
                  "pvc-name": "{{tasks.create-pq-values-pvc.outputs.parameters.pvc-name}}"
                },
                { "analysis_type": "genrich_pileups",
                  "prefix": "pileups-",
                  "suffix": ".txt",
                  "pvc-name": "{{tasks.create-pileups-pvc.outputs.parameters.pvc-name}}" 
                }
              ]

          # Maybe here assert that output files exist in S3 before updating task marker
          # Metadata computed from PVCs, output-files-post-processing doesn't assure us the files made it to S3

          - name: update-peak-calling-task-marker
            depends: output-files-post-processing.Succeeded
            templateRef:
              name: update-task-marker-v1-submittable
              template: update-task-marker
            arguments:
              parameters:
                - name: task-marker-s3-key
                  value: "{{tasks.compute-peak-calling-task-marker-name.outputs.result}}"
                - name: task-payload
                  value: "{{inputs.parameters.task-payload}}"

          - name: delete-pq-values-pvc
            depends: output-files-post-processing.Succeeded
            templateRef:
              name: delete-patch-pvc-kubectl-v1-submittable
              template: delete-patch-pvc-kubectl
            arguments:
              parameters:
                - name: pvc-name
                  value: "{{tasks.create-pq-values-pvc.outputs.parameters.pvc-name}}"
                - name: kubeconfig-path
                  value: "{{inputs.parameters.kubeconfig-path}}"

          - name: delete-pileups-pvc
            depends: output-files-post-processing.Succeeded
            templateRef:
              name: delete-patch-pvc-kubectl-v1-submittable
              template: delete-patch-pvc-kubectl
            arguments:
              parameters:
                - name: pvc-name
                  value: "{{tasks.create-pileups-pvc.outputs.parameters.pvc-name}}"
                - name: kubeconfig-path
                  value: "{{inputs.parameters.kubeconfig-path}}"

    - name: dag-merge-controls
      inputs:
        parameters:
          - name: controls-merge-tasks
          - name: pvc-name
      dag:
        tasks:
          - name: merge-controls
            template: samtools-merge-controls-cmd
            arguments:
              parameters:
                - name: alignments-filenames
                  value: "{{=sprig.join(' ', map(jsonpath(item.alignments, '$.filename'), { 'so_queryname_' + # }))}}"
                - name: pvc-name
                  value: "{{inputs.parameters.pvc-name}}"
                - name: merged-alignment-filename
                  value: "so_queryname_{{=jsonpath(item.merged_alignment, '$.filename')}}"
            withParam: "{{inputs.parameters.controls-merge-tasks}}"


    - name: samtools-merge-controls-cmd
      retryStrategy:
        limit: "5"
        retryPolicy: "OnError"
      inputs:
        parameters:
          # Source
          - name: alignments-filenames
          # Destination
          - name: pvc-name
          - name: merged-alignment-filename

          - name: num-threads
            value: "8"
      volumes:
        - name: workdir
          persistentVolumeClaim:
            claimName: '{{inputs.parameters.pvc-name}}'
      container:
        workingDir: /mnt/vol
        image: ghcr.io/daugo/ensembl-reg-samtools:latest
        imagePullPolicy: Always
        command: [ bash, -c, -ue, -o, xtrace ]
        args: [ "ls -altrh \
               && rm -f {{inputs.parameters.merged-alignment-filename}} \
               && samtools merge \
               -@ {{inputs.parameters.num-threads}} \
               -n \
               -o - \
               {{inputs.parameters.alignments-filenames}} \
               | samtools view \
               -@ {{inputs.parameters.num-threads}} \ 
               -b \
               -o {{inputs.parameters.merged-alignment-filename}} \
               && samtools quickcheck {{inputs.parameters.merged-alignment-filename}} \
               && ls -altrh " ]
        volumeMounts:
          - name: workdir
            mountPath: /mnt/vol
        resources:
          limits:
            cpu: 8100m
            memory: 30Gi
          requests:
            cpu: 7800m
            memory: 25Gi


    - name: dag-samtools-sort-tasks
      inputs:
        parameters:
          - name: alignments-info
          - name: kubeconfig-path
      outputs:
        parameters:
          - name: pvcs-names
            valueFrom:
              parameter: "{{tasks.create-tasks.outputs.parameters.pvc-name}}"
      dag:
        tasks:
          - name: create-tasks
            template: dag-samtools-sort-task
            arguments:
              parameters:
                - name: alignment-info
                  value: "{{item}}"
                - name: kubeconfig-path
                  value: "{{inputs.parameters.kubeconfig-path}}"
            withParam: "{{inputs.parameters.alignments-info}}"


    - name: dag-samtools-sort-task
      inputs:
        parameters:
          - name: alignment-info
          - name: kubeconfig-path
      outputs:
        parameters:
          - name: pvc-name
            valueFrom:
              parameter: "{{tasks.create-pvc.outputs.parameters.pvc-name}}"
      dag:
        tasks:
          - name: get-pvc-size
            templateRef:
              name: compute-pvc-size-v1-submittable
              template: compute-pvc-size
            arguments:
              parameters:
                - name: associated_files_size
                  value: "{{=jsonpath(inputs.parameters['alignment-info'], '$.file_size')}}"
                - name: size_factor
                  value: "5.5"

          - name: create-pvc
            depends: get-pvc-size.Succeeded
            templateRef:
              name: create-pvc-kubectl-v1-submittable
              template: create-pvc-kubectl
            arguments:
              parameters:
                - name: pvc-size
                  value: "{{tasks.get-pvc-size.outputs.parameters.pvc-size-formatted}}"
                - name: kubeconfig-path
                  value: "{{inputs.parameters.kubeconfig-path}}"

          - name: samtools-sort-task
            depends: create-pvc.Succeeded
            templateRef:
              name: sort-alignment-by-queryname-pvc-callable-v2-submittable
              template: sort-alignment-by-queryname-pvc-callable
            arguments:
              parameters:
                - name: s3-key
                  value: "{{=jsonpath(inputs.parameters['alignment-info'], '$.s3_key')}}"
                - name: pvc-name
                  value: "{{tasks.create-pvc.outputs.parameters.pvc-name}}"
                - name: kubeconfig-path
                  value: "{{inputs.parameters.kubeconfig-path}}"


    - name: dag-copy-sort-alignments-pvcs-to-peak-calling-pvc
      parallelism: 1
      inputs:
        parameters:
          - name: source-pvcs
          - name: destination-pvc
      dag:
        tasks:
          - name: sort-alignment-pvc-to-peak-calling-pvc-copy
            templateRef:
              name: pvc-to-pvc-copy-v1-submittable
              template: pvc-to-pvc-copy
            arguments:
              parameters:
                - name: source-pvc
                  value: "{{item}}"
                - name: source-path
                  value: "*bam"
                - name: destination-pvc
                  value: "{{inputs.parameters.destination-pvc}}"
            withParam: "{{inputs.parameters.source-pvcs}}"


    - name: dag-output-files-post-processing
      parallelism: 2
      inputs:
        parameters:
          - name: pvc-name
          - name: task-payload
          - name: analysis-type-info
      dag:
        tasks:
          - name: compute-output-file-metadata
            templateRef:
              name: compute-file-metadata-pvc-callable-v1-submittable
              template: compute-file-metadata-pvc-callable
            arguments:
              parameters:
                - name: pvc-name
                  value: "{{inputs.parameters.pvc-name}}"
                - name: filename
                  value: "{{=jsonpath(inputs.parameters['analysis-type-info'], '$.prefix')}}\
                        {{=jsonpath(inputs.parameters['task-payload'], '$.out_basename')}}\
                        {{=jsonpath(inputs.parameters['analysis-type-info'], '$.suffix')}}"

          - name: get-output-file-post-request-payload
            depends: compute-output-file-metadata.Succeeded
            templateRef:
              name: analysis-file-post-request-payload-v1-submittable
              template: analysis-file-post-request-payload
            arguments:
              parameters:
                - name: s3-key
                  value: "{{=jsonpath(inputs.parameters['task-payload'], '$.out_s3_prefix')}}\
                          {{=jsonpath(inputs.parameters['analysis-type-info'], '$.prefix')}}\
                          {{=jsonpath(inputs.parameters['task-payload'], '$.out_basename')}}\
                          {{=jsonpath(inputs.parameters['analysis-type-info'], '$.suffix')}}\
                          .tgz"
                - name: basename
                  value: "{{=jsonpath(inputs.parameters['analysis-type-info'], '$.prefix')}}\
                          {{=jsonpath(inputs.parameters['task-payload'], '$.out_basename')}}\
                          {{=jsonpath(inputs.parameters['analysis-type-info'], '$.suffix')}}"
                - name: file-size
                  value: "{{tasks.compute-output-file-metadata.outputs.parameters.file-size}}"
                - name: md5sum
                  value: "{{tasks.compute-output-file-metadata.outputs.parameters.md5sum}}"
                - name: experiment-id
                  value: "{{=jsonpath(inputs.parameters['task-payload'], '$.experiment_id')}}"
                - name: assembly-id
                  value: "{{=jsonpath(inputs.parameters['task-payload'], '$.assembly_id')}}"
                - name: analysis-type
                  value: "{{=jsonpath(inputs.parameters['analysis-type-info'], '$.analysis_type')}}"

          - name: compress-file
            depends: compute-output-file-metadata.Succeeded
            templateRef:
              name: compress-tgz-pvc-callable-v1-submittable
              template: compress-tgz-pvc-callable-pigz
            arguments:
              parameters:
                - name: pvc-name
                  value: "{{inputs.parameters.pvc-name}}"
                - name: filename
                  value: "{{=jsonpath(inputs.parameters['analysis-type-info'], '$.prefix')}}\
                          {{=jsonpath(inputs.parameters['task-payload'], '$.out_basename')}}\
                          {{=jsonpath(inputs.parameters['analysis-type-info'], '$.suffix')}}"
                - name: out-filename
                  value: "{{=jsonpath(inputs.parameters['analysis-type-info'], '$.prefix')}}\
                          {{=jsonpath(inputs.parameters['task-payload'], '$.out_basename')}}\
                          {{=jsonpath(inputs.parameters['analysis-type-info'], '$.suffix')}}\
                          .tgz"

          - name: save-file-to-s3
            depends: compress-file.Succeeded
            templateRef:
              name: from-pvc-to-s3-object-v1-submittable
              template: from-pvc-to-s3-object
            arguments:
              parameters:
                - name: pvc-name
                  value: "{{inputs.parameters.pvc-name}}"
                - name: filename
                  value: "{{tasks.compress-file.outputs.parameters.out-filename}}"
                - name: s3-key
                  value: "{{=jsonpath(inputs.parameters['task-payload'], '$.out_s3_prefix')}}\
                          {{=jsonpath(inputs.parameters['analysis-type-info'], '$.prefix')}}\
                          {{=jsonpath(inputs.parameters['task-payload'], '$.out_basename')}}\
                          {{=jsonpath(inputs.parameters['analysis-type-info'], '$.suffix')}}\
                          .tgz"
                - name: cpu-limit
                  value: "6000m"
                - name: memory-limit
                  value: "20Gi"

    - name: resolve-genrich-inputs
      retryStrategy:
        limit: "5"
        backoff:
          duration: "10s"
          factor: "2"
        retryPolicy: "OnError"
      inputs:
        parameters:
          - name: signals-payload
          - name: controls-payload
            value: ""
          - name: similarity-threshold
            value: "0.1"
      script:
        image: ghcr.io/daugo/ensembl-reg-python:latest
        imagePullPolicy: Always
        command: [ python ]
        source: |
          import json
          import logging
          import operator
          import re
          import sys
          from collections import defaultdict
          from functools import partial
          from pathlib import Path
          from typing import (
              Any,
          )
          
          from pydantic import BaseModel
          from rich.logging import RichHandler
          from rich.pretty import pretty_repr
          
          custom_pretty_repr = partial(pretty_repr, indent_size=2)
          
          
          def _setup_logging():
              logging.basicConfig(
                  format="[ %(asctime)s ] — [ %(funcName)s:%(lineno)d ] — %("
                  "message)s",
                  level=logging.INFO,
                  datefmt="%Y-%m-%d %H:%M:%S",
                  handlers=[
                      RichHandler(
                          rich_tracebacks=True,
                          tracebacks_show_locals=True,
                          show_time=False,
                      )
                  ],
              )
          
          
          def get_avg_read_length_from_run_type_str(run_type_str: str) -> int | None:
              try:
                  return int(
                      re.search(
                          r"([0-9]+)",
                          run_type_str,
                      ).group(0)
                  )
              except TypeError:
                  return None
          
          
          def get_library_type_from_run_type_str(run_type_str: str) -> str | None:
              if run_type_str.startswith("PE"):
                  return "PE"
              elif run_type_str.startswith("SE"):
                  return "SE"
              else:
                  raise ValueError("Unknown library type")
          
          
          def get_merged_controls_filename(filenames: list[str]) -> str:
              filename_bits = [Path(f_n).stem for f_n in filenames]
          
              return f"{'-'.join(filename_bits)}.bam"
          
          
          def relative_distance(a: int, b: int) -> float:
              return abs(a - b) / max(a, b)
          
          
          def group_read_lengths_by_similarity(
              numbers: list[int],
              threshold: float = 0.1,
          ) -> list[list[int]]:
              numbers.sort()
          
              # Initialize the first group
              groups = [[numbers[0]]]
          
              for number in numbers[1:]:
                  # Calculate the relative distance to the last number in the current
                  # group
                  distance = relative_distance(number, groups[-1][-1])
          
                  if distance <= threshold:
                      groups[-1].append(number)
                  else:
                      groups.append([number])
          
              return groups
          
          
          class RunTypeCategory(BaseModel):
              library_type: str
              interval: tuple[int, int]
          
              @property
              def run_type_category(self) -> str:
                  if self.interval[0] != self.interval[1]:
                      return (
                          f"{self.library_type}{self.interval[0]}-{self.interval[1]}nt"
                      )
          
                  return f"{self.library_type}{self.interval[0]}nt"
          
          
          class AlignmentInfo(BaseModel):
              run_type: str
              filename: str
          
              @property
              def library_type(self) -> str:
                  return get_library_type_from_run_type_str(self.run_type)
          
              @property
              def avg_read_length(self) -> int:
                  return get_avg_read_length_from_run_type_str(self.run_type)
          
          
          class SignalControlPair(BaseModel):
              signal: AlignmentInfo
              control: AlignmentInfo | None
          
          
          class MergeAlignmentTask(BaseModel):
              alignments: list[AlignmentInfo]
              merged_alignment: AlignmentInfo | None
          
          
          def get_run_types_categories(
              alignments: list[AlignmentInfo],
          ) -> list[RunTypeCategory]:
              """
              Identify run types categories for a list of alignments
              """
              run_types_categories: list[RunTypeCategory] = []
              library_type_aln: dict[str, AlignmentInfo | Any] = defaultdict(list)
          
              for aln in alignments:
                  library_type_aln[aln.library_type].append(aln)
          
              for library_type, alignments in library_type_aln.items():
                  groups = group_read_lengths_by_similarity(
                      [aln.avg_read_length for aln in alignments]
                  )
                  for g in groups:
                      run_types_categories.append(
                          RunTypeCategory(
                              library_type=library_type,
                              interval=(min(g), max(g)),
                          )
                      )
          
              return run_types_categories
          
          
          def resolve_controls(
              control_alignments: list[AlignmentInfo],
              controls_run_type_categories: list[RunTypeCategory],
          ) -> tuple[dict[str, AlignmentInfo], list[MergeAlignmentTask]]:
              """
              Merge control names based on run type categories
              """
              run_types_category_control_filenames: dict[
                  str, list[AlignmentInfo]
              ] = defaultdict(list)
          
              run_type_category_final_control_aln: dict[str, AlignmentInfo] = {}
          
              for c in control_alignments:
                  for rtc in controls_run_type_categories:
                      if (
                          rtc.interval[0] <= c.avg_read_length <= rtc.interval[1]
                          and c.library_type == rtc.library_type
                      ):
                          run_types_category_control_filenames[
                              rtc.run_type_category
                          ].append(c)
          
              merged_tasks = []
              for (
                  run_type_category,
                  c_alignments,
              ) in run_types_category_control_filenames.items():
                  if len(c_alignments) > 1:
                      # Important to sort the alignments by avg_read_length and
                      # filename to ensure reproducibility
                      c_alignments.sort(
                          key=operator.attrgetter(
                              "library_type",
                              "avg_read_length",
                              "filename",
                          )
                      )
                      filename_bits = [
                          Path(c_aln.filename).stem for c_aln in c_alignments
                      ]
                      filename_merged = f"{'-'.join(filename_bits)}.bam"
          
                      merged_tasks.append(
                          MergeAlignmentTask(
                              alignments=c_alignments,
                              merged_alignment=AlignmentInfo(
                                  filename=filename_merged, run_type=run_type_category
                              ),
                          )
                      )
                      run_type_category_final_control_aln[
                          run_type_category
                      ] = AlignmentInfo(
                          filename=filename_merged,
                          run_type=run_type_category,
                      )
          
                  else:
                      assert (
                          len(c_alignments) == 1
                      ), "More than one control alignment not expected here"
                      run_type_category_final_control_aln[run_type_category] = (
                          c_alignments
                      )[0]
          
              return run_type_category_final_control_aln, merged_tasks
          
          
          def map_signals_to_controls(
              signal_alignments: list[AlignmentInfo],
              control_run_types_categories: list[RunTypeCategory],
              run_type_category_final_control_alignments: dict[str, AlignmentInfo],
              similarity_threshold: float = 0.1,
          ) -> list[SignalControlPair]:
              """
              Map signals to controls
              """
          
              signal_control_pairs: list[SignalControlPair] = []
          
              for s in signal_alignments:
                  control_found = False
                  for rtc in control_run_types_categories:
                      if (
                          (rtc.interval[0] <= s.avg_read_length <= rtc.interval[1])
                          or (
                              relative_distance(s.avg_read_length, rtc.interval[0])
                              <= similarity_threshold
                              or relative_distance(s.avg_read_length, rtc.interval[1])
                              <= similarity_threshold
                          )
                      ) and s.library_type == rtc.library_type:
                          signal_control_pairs.append(
                              SignalControlPair(
                                  signal=s,
                                  control=run_type_category_final_control_alignments[
                                      rtc.run_type_category
                                  ],
                              )
                          )
                          control_found = True
                          break
                  if not control_found:
                      signal_control_pairs.append(
                          SignalControlPair(
                              signal=s,
                              control=None,
                          )
                      )
          
              return signal_control_pairs
          
          
          def main() -> int:
              _setup_logging()
          
              similarity_threshold = "{{inputs.parameters.similarity-threshold}}"
              signals_payload_json = """{{inputs.parameters.signals-payload}}"""
              controls_payload_json = """{{inputs.parameters.controls-payload}}"""
          
              similarity_threshold = float(similarity_threshold)
          
              signals_payload_data = json.loads(signals_payload_json)
          
              logging.info(
                  f"signals_payload_data:\n"
                  f"{custom_pretty_repr(signals_payload_data)}"
              )
          
              signal_alignments = [AlignmentInfo(**s) for s in signals_payload_data]
          
              # Important to sort the alignments by avg_read_length and
              # filename to ensure reproducibility
              signal_alignments.sort(
                  key=operator.attrgetter(
                      "library_type",
                      "avg_read_length",
                      "filename",
                  )
              )
          
              logging.info(
                  f"signal_alignments:\n{custom_pretty_repr(signal_alignments)}"
              )
          
              if controls_payload_json:
                controls_payload_data = json.loads(controls_payload_json)
              else:
                controls_payload_data = []
          
              logging.info(
                  f"controls_payload_data:\n{custom_pretty_repr(controls_payload_data)}"
              )
          
              control_alignments = [AlignmentInfo(**c) for c in controls_payload_data]
              logging.info(
                  f"control_alignments:\n{custom_pretty_repr(control_alignments)}"
              )
          
              if control_alignments:
                  control_run_types_categories = get_run_types_categories(
                      control_alignments
                  )
          
                  logging.info(
                      f"control_run_types_categories:\n"
                      f"{custom_pretty_repr(control_run_types_categories)}"
                  )
          
                  run_type_category_final_control_aln, merge_tasks = resolve_controls(
                      control_alignments,
                      control_run_types_categories,
                  )
                  logging.info(
                      f"run_type_category_final_control_aln:\n"
                      f" {custom_pretty_repr(run_type_category_final_control_aln)}"
                  )
                  logging.info(f"merge_tasks:\n{custom_pretty_repr(merge_tasks)}")
                  signal_control_pairs = map_signals_to_controls(
                      signal_alignments,
                      control_run_types_categories,
                      run_type_category_final_control_aln,
                      similarity_threshold=similarity_threshold,
                  )
          
                  logging.info(
                      f"signal_control_pairs:\n"
                      f"{custom_pretty_repr(signal_control_pairs)}"
                  )
          
                  signals_formatted = [
                      f"so_queryname_{p.signal.filename}" for p in signal_control_pairs
                  ]
                  controls_formatted = [
                      f"so_queryname_{p.control.filename}" if p.control else "null"
                      for p in signal_control_pairs
                  ]
              else:
                  signals_formatted = [
                      f"so_queryname_{s.filename}" for s in signal_alignments
                  ]
                  controls_formatted = ["null" for _ in signal_alignments]
          
                  merge_tasks = []
          
              with open("signals_files_formatted_str.txt", "w") as f:
                  f.write(",".join(signals_formatted))
          
              with open("controls_files_formatted_str.txt", "w") as f:
                  f.write(
                      '""'
                      if all(c == "null" for c in controls_formatted)
                      else ",".join(controls_formatted)
                  )
          
              with open("merged_tasks.json", "w") as f:
                  f.write(json.dumps([t.dict() for t in merge_tasks]))
          
              # TODO: This could be inferred from the merge_tasks list being empty or not.
              # However, performing this check using expr and/or sprig is not working for 'when'
              # checks. Related issue: https://github.com/argoproj/argo-workflows/issues/7576
              with open("merged_tasks_present.txt", "w") as f:
                  f.write("true" if merge_tasks else "false")
          
              return 0
          
          
          if __name__ == "__main__":
              sys.exit(main())

      outputs:
        parameters:
          - name: signals_files_formatted_str
            valueFrom:
              path: signals_files_formatted_str.txt
          - name: controls_files_formatted_str
            valueFrom:
              path: controls_files_formatted_str.txt
          - name: merge-tasks
            valueFrom:
              path: merged_tasks.json
          # TODO: This could be inferred from the merge_tasks list being empty or not.
          # However, performing this check using expr and/or sprig is not working for 'when'
          # checks. Related issue: https://github.com/argoproj/argo-workflows/issues/7576
          - name: merge-tasks-present
            valueFrom:
              path: merged_tasks_present.txt
